{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af04aa9",
   "metadata": {},
   "source": [
    "# YOLO V8 PEOPLE COUNT AND HELMET DETECTION  IN TRAFFIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1c9942",
   "metadata": {},
   "source": [
    "Collect Images From Video Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1043d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def video_to_frames(video_path, output_folder, frame_interval=1):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Capture the video from the given path\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    saved_frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Read the frame from the video\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If there are no more frames, break the loop\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Save the frame only if it's the nth frame\n",
    "        if count % frame_interval == 0:\n",
    "            frame_filename = os.path.join(output_folder, f\"frame_4{saved_frame_count:05d}.jpeg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            saved_frame_count += 1\n",
    "\n",
    "        # Increase the frame count\n",
    "        count += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    print(f\"Frames extracted and saved in {output_folder}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "video_path = '/home/manikkamani/Downloads/test_s1/3553898991-preview.mp4'\n",
    "output_folder = '/home/manikkamani/Downloads/test_s1/test_data'\n",
    "\n",
    "# Extract every 8th frame\n",
    "video_to_frames(video_path, output_folder, frame_interval=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafebbe",
   "metadata": {},
   "source": [
    "Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e031780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the path to your image folder\n",
    "image_folder = '/home/manikkamani/Downloads/test_s1/test_data'\n",
    "\n",
    "# List all image files in the folder\n",
    "image_files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
    "\n",
    "# Number of images to randomly select\n",
    "num_images_to_augment = int(len(image_files)*0.3)\n",
    "\n",
    "# Randomly select image files\n",
    "selected_files = random.sample(image_files, num_images_to_augment)\n",
    "\n",
    "\n",
    "# Define the additional augmentations\n",
    "additional_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.2),  # Adjust brightness\n",
    "    transforms.RandomHorizontalFlip(),  # Optional: Random horizontal flip\n",
    "    transforms.ToTensor()  # Convert to tensor\n",
    "])\n",
    "\n",
    "# Function to apply additional transformations\n",
    "def process_image(image_path):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Apply additional transformations\n",
    "    augmented_image = additional_transform(image)\n",
    "    \n",
    "    return augmented_image\n",
    "\n",
    "# Process and save/display the images\n",
    "for file in selected_files:\n",
    "    image_path = os.path.join(image_folder, file)\n",
    "    augmented_image = process_image(image_path)\n",
    "    \n",
    "    # Convert tensor back to PIL image for display\n",
    "    to_pil = transforms.ToPILImage()\n",
    "    augmented_image_pil = to_pil(augmented_image)\n",
    "    \n",
    "    # Save or display the augmented image\n",
    "    augmented_image_pil.save(f'{image_folder}/_augmented_{file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91b0e8",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87abefe",
   "metadata": {},
   "source": [
    "Split Data in to Train Test Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9bda76fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(image_dir, label_dir, output_dir, train_size=0.7, val_size=0.2):\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Create directories for splits\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for subdir in ['images', 'labels']:\n",
    "            os.makedirs(os.path.join(output_dir, subdir, split), exist_ok=True)\n",
    "\n",
    "    # List all files\n",
    "    all_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]  # Assuming .jpg images\n",
    "    random.shuffle(all_files)  # Shuffle files to ensure random split\n",
    "\n",
    "    # Calculate number of samples for each split\n",
    "    total_files = len(all_files)\n",
    "    train_end = int(train_size * total_files)\n",
    "    val_end = int((train_size + val_size) * total_files)\n",
    "\n",
    "    # Split data\n",
    "    train_files = all_files[:train_end]\n",
    "    val_files = all_files[train_end:val_end]\n",
    "    test_files = all_files[val_end:]\n",
    "\n",
    "    # Function to move files\n",
    "    def move_files(file_list, split):\n",
    "        for file_name in file_list:\n",
    "            # Move image\n",
    "            src_image = os.path.join(image_dir, file_name)\n",
    "            dst_image = os.path.join(output_dir, 'images', split, file_name)\n",
    "            shutil.copy(src_image, dst_image)\n",
    "            \n",
    "            # Move label\n",
    "            label_name = file_name.replace('.jpg', '.txt')\n",
    "            src_label = os.path.join(label_dir, label_name)\n",
    "            dst_label = os.path.join(output_dir, 'labels', split, label_name)\n",
    "            if os.path.exists(src_label):\n",
    "                shutil.copy(src_label, dst_label)\n",
    "\n",
    "    # Move files to respective directories\n",
    "    move_files(train_files, 'train')\n",
    "    move_files(val_files, 'val')\n",
    "    move_files(test_files, 'test')\n",
    "\n",
    "# Example usage\n",
    "image_directory = '/home/manikkamaDownloads/test_s1/test_data'\n",
    "label_directory = '/home/manikkamani/Downloads/test_s1/test_data'\n",
    "output_directory = '/home/manikkamani/Downloads/test_s1/data2'\n",
    "\n",
    "split_dataset(image_directory, label_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e93f7",
   "metadata": {},
   "source": [
    "Create Training Job Config YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f38961f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML configuration file saved to /home/manikkamani/Downloads/test_s1/data2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "def create_yaml(dataset_dir, num_classes, class_names, output_yaml):\n",
    "\n",
    "    # Paths to dataset splits\n",
    "    train_path = os.path.join(dataset_dir, 'images', 'train')\n",
    "    val_path = os.path.join(dataset_dir, 'images', 'val')\n",
    "    test_path = os.path.join(dataset_dir, 'images', 'test')\n",
    "\n",
    "    # Prepare YAML content\n",
    "    yaml_content = {\n",
    "        'train': train_path,\n",
    "        'val': val_path,\n",
    "        'test': test_path,\n",
    "        'nc': num_classes,\n",
    "        'names': class_names\n",
    "    }\n",
    "\n",
    "    # Write YAML content to file\n",
    "    with open(f'{output_yaml}/data.yaml', 'w') as file:\n",
    "        yaml.safe_dump(yaml_content, file, sort_keys=False)\n",
    "\n",
    "    print(f'YAML configuration file saved to {output_yaml}')\n",
    "\n",
    "\n",
    "class_names_list = ['person', 'rider', 'with_helmet', 'without_helmet']  # Replace with your actual class names\n",
    "number_of_classes = len(class_names_list)\n",
    "\n",
    "\n",
    "create_yaml(image_directory, number_of_classes, class_names_list, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1db628d",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLOv8 model\n",
    "model = YOLO('yolov8s.pt')  # Replace with yolov8m.pt, yolov8l.pt, or yolov8x.pt for larger models\n",
    "\n",
    "# Train the model on your dataset\n",
    "model.train(data='dataset/data.yaml', epochs=100, imgsz=640, lr0=0.01, batch=8, weight_decay=0.0005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85fd14",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e2c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    " \n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(\"runs/detect/train7/weights/best.pt\")\n",
    " \n",
    "# Open the video file\n",
    "video_path = \"test_videos/3418603233-preview.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    " \n",
    "# Define class labels\n",
    "rider_label = \"rider\"\n",
    "people_label = \"people\"\n",
    "without_helmate_label = 'without_helmate'\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    " \n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame)\n",
    "        \n",
    "        # Extract the detections\n",
    "        detections = results[0].boxes\n",
    "        \n",
    "        # Initialize counters\n",
    "        rider_count = 0\n",
    "        people_count = 0\n",
    "        without_helmate = 0\n",
    "        \n",
    "        # Count detections\n",
    "        for box in detections:\n",
    "            class_id = int(box.cls)  # Get the class ID for this detection\n",
    "            class_name = results[0].names[class_id]  # Get the class name from the class ID\n",
    "\n",
    "            if class_name == rider_label:\n",
    "                rider_count += 1\n",
    "            elif class_name == people_label:\n",
    "                people_count += 1\n",
    "            elif class_name == without_helmate_label:\n",
    "                without_helmate += 1\n",
    "            \n",
    " \n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "        \n",
    "        # Add text to display counts\n",
    "        cv2.putText(annotated_frame, f\"{without_helmate_label.capitalize()} Count: {without_helmate}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "        cv2.putText(annotated_frame, f\"{people_label.capitalize()} Count: {people_count+rider_count}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    " \n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"Inference\", annotated_frame)\n",
    " \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f151106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcbdcf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
